# VADER sentiment analysis and LDA Topic modelling on Subreddit data
 The current project does sentiment analysis on some of the online posts made by Barcelona fans to establish the general opinion about the club using Valence Aware Dictionary for sEntiment Reasoning (VADER), then using Latent Dirichlet Allocation (LDA), it does topic modeling to reveal the topics in the posts. The process also involved using existing python modules and packages to return (predict) sentiments and topics from the available data.
VADER is a rule-based sentiment analysis library that was designed with a focus on social media. It addresses the nature of social media text; that is, it is designed to capture short sentences with emojis, varied punctuations, repetitive vocabulary, and others (Rao, 2019). VADER provides a unique opportunity for sentiment analysis is since it is sensitive to polarity; that is, it can calculate the strength of the emotion, either positive or negative (Hutto & Gilbert, 2014). VADER accounts for capitalizations, emojis, and exclamation marks to calculate the intensity towards the positive or negative. VADER takes a string as the input and returns a dictionary of polarity scores. The output has a score for negative, neutral, and positive, and compound (calculated from the other polarity scores). The compound is standardized on a scale of range [-1,1]. VADER is also intelligent enough to understand the context of phrases such as “did not love” such that it understands that that is a negative sentiment instead of relying on individual lexical items. The current project does not strive to do a fine-grained analysis of the degree of polarity for individual posts but instead determine whether the posts are generally positive or negative (see also Borg & Boldt, 2020).
This project uses textual data from a Barcelona subreddit community, r/Barca, with approximately 90,000 members. The community is dedicated to all things Barcelona  and has been active since 2010. To get started, the project scrapes data from Reddit using the Python Reddit API Wrapper (PRAW). After installing PRAW, the next step is to create an app within Reddit that gives us the information needed to authenticate the user, such that one can access the platform (see code). Once this step is completed, one is now able to scrape data from a subreddit. 
 
Data scraping from Reddit gets one piece of information about the post (the title and body), the number of comments, up-votes or down-votes to the posts and comments, among other types of information. The number of up-or down-votes helps calculate the post’s score, which could provide further information about the number of people who agree or disagree with the posts (or the sentiment of those posts). It is essential to mention that the project focuses on the top subreddit posts (a limit of 1000 top posts ). The rationale for this decision is that the popular posts have more engagement, which can offer insights into the nature of the conversation . It is suspected that many of the popular posts correspond to significant events  around the team. Sentiment analysis will provide the general feelings of the fans about those events.
Loading the data to a data frame reveals that the most liked (upvoted) posts (with a score of 2500-3000) also have many comments, ranging from 100-200 comments. Noticeably too, not all post titles have an accompanying body included in the body column. Additionally, cognizant that it could be challenging to include all the post comments in the same data frame as the post titles and body, a separate comments data frame was created. The following steps include data cleaning and preprocessing the available data to make it ready for processing. Data from both data frames were first converted to strings, then using python functions and packages, they were split into words, then tokenized using nltk.tokenize. English stopwords and punctuation were also removed after the tokens were all converted to the lowercase.
Subsequently, this project reiterated the cleaned data over the VADER library, as the code snippet below illustrates. Further analysis of the data involved doing an LDA analysis, revealing any hidden topics (Jacobi et al., 2016; Onan et al., 2016). Understanding governing this type of modeling is that after arranging documents based on latent topics, word distribution can then be modeled (Bagheri et al., 2014). As Tong & Zhang (2016) state, these topics contain a fixed vocabulary made of high probability lexical items.
 
The model training process requires the gensim package LdaModel with its package dependencies (models, corpora, and others) to be loaded. An LDA model of Barcelona top subreddit posts and comments returned after 100 iterations with num_topics = five topics.

